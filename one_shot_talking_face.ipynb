{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/one-shot-talking-face/blob/main/one_shot_talking_face.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "%cd /content\n",
        "!git clone https://huggingface.co/camenduru/pocketsphinx\n",
        "%cd  /content/pocketsphinx\n",
        "!sudo cmake --build build --target install\n",
        "\n",
        "%cd /content\n",
        "!git clone https://huggingface.co/camenduru/one-shot-talking-face\n",
        "%cd /content/one-shot-talking-face\n",
        "!pip install -r /content/one-shot-talking-face/requirements.txt\n",
        "!chmod 755 /content/one-shot-talking-face/OpenFace/FeatureExtraction\n",
        "!mkdir /content/out\n",
        "!apt install libgtk2.0-0 jq -y\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, torchaudio\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def show_video(video_path, video_width = 256):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "wav_file = \"/content/emma1.wav\"  #@param {type:\"string\"}\n",
        "image_file = \"/content/test5.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(wav_file)\n",
        "torchaudio.save(wav_file, waveform, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
        "\n",
        "os.environ['wav_file'] = wav_file\n",
        "os.environ['image_file'] = image_file\n",
        "random_int = str(random.randint(1, 1000000))\n",
        "\n",
        "!mkdir /content/train\n",
        "!cp $wav_file /content/train/audio.wav\n",
        "!cp $image_file /content/train/image.png\n",
        "\n",
        "!pocketsphinx -phone_align yes single /content/train/audio.wav $text | jq '[.w[]|{word: (.t | ascii_upcase | sub(\"<S>\"; \"sil\") | sub(\"<SIL>\"; \"sil\") | sub(\"\\\\(2\\\\)\"; \"\") | sub(\"\\\\(3\\\\)\"; \"\") | sub(\"\\\\(4\\\\)\"; \"\") | sub(\"\\\\[SPEECH\\\\]\"; \"SIL\") | sub(\"\\\\[NOISE\\\\]\"; \"SIL\")), phones: [.w[]|{ph: .t | sub(\"\\\\+SPN\\\\+\"; \"SIL\") | sub(\"\\\\+NSN\\\\+\"; \"SIL\"), bg: (.b*100)|floor, ed: (.b*100+.d*100)|floor}]}]' > /content/test.json\n",
        "%cd /content/one-shot-talking-face\n",
        "!python -B test_script.py --img_path /content/train/image.png --audio_path /content/train/audio.wav --phoneme_path /content/test.json --save_dir /content/train\n",
        "\n",
        "os.environ['ran_num'] = random_int\n",
        "!cp /content/train/image_audio.mp4 \"/content/out/${ran_num}.mp4\"\n",
        "\n",
        "clear_output()\n",
        "\n",
        "show_video(f\"/content/out/{random_int}.mp4\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
